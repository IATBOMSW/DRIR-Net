# MinkLoc3D model
[MODEL]
model = MinkFPN
pooling = GeM
version = MinkLoc3D-S
mink_quantization_size = 2.5,2.0,1.875
planes = 32,64,64
layers = 1,1,1
num_top_down = 1
conv0_kernel_size = 5
block=BasicBlock
feature_size = 256
output_dim = 256
gpu = 0


[POINTNET]
with_pointnet = False
pnts = False

[PPLNET]
with_pplnet = True
t_net = False
sectors = 256
npoint = 32
knn = 3
divide = 4
p_num_reduce = 1
fusion = True
pw_rat = 1.0
vw_rat = 0.0

# fusion = True pw_rat = 1.0 vw_rat = 1.0 for normal
# fusion = True pw_rat = 0.8 vw_rat = 0.2 or pw_rat = 1.0 vw_rat = 0.0 for rotation

[CROSS-ATTENTION]
with_cross_att = True
num_heads = 1
d_feedforward = 64
dropout = 0
transformer_act = relu
pre_norm = True
attention_type = linear_attention
sa_val_has_pos_emb = True
ca_val_has_pos_emb = True
num_encoder_layers = 1
transformer_encoder_has_pos_emb = True

