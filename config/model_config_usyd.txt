[MODEL]
model = MinkFPN
pooling = GeM
version = MinkLoc3D-S
mink_quantization_size = 2.5,2.0,1.875
#version = MinkLoc3D
#mink_quantization_size = 1.0,1.0,1.0
planes = 32,64,64
layers = 1,1,1
num_top_down = 1
conv0_kernel_size = 5
block=BasicBlock
feature_size = 256
output_dim = 256
gpu = 0


[POINTNET]
with_pointnet = False
pnts = False

[PPLNET]
with_pplnet = True
t_net = False
sectors = 256
npoint = 32
knn = 5

divide = 4
p_num_reduce = 1
fusion_argu = True
fusion = True
pw_rat = 1.0
vw_rat = 1.0

[CROSS-ATTENTION]
with_cross_att = True
num_heads = 1
d_feedforward = 64
dropout = 0
transformer_act = relu
pre_norm = True
# dot_prod linear_attention
attention_type = linear_attention
sa_val_has_pos_emb = True
ca_val_has_pos_emb = True
num_encoder_layers = 1
transformer_encoder_has_pos_emb = True

[SELF-ATTENTION]
with_self_att = FALSE
num_layers = 3
linear_att = False
kernel_size = 3
stride = 1
dilation = 1
num_heads = 8
